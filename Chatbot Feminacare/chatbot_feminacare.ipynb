{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQfOuwL0xsMg"
      },
      "source": [
        "### Input library"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O dataset3.json https://raw.githubusercontent.com/Wiradhika6051/Feminicare/ML/chatbot/dataset3.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbSXq7ApEO1Q",
        "outputId": "65d38281-8bc0-4fe6-9f24-68d07e11aedd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-12 13:32:37--  https://raw.githubusercontent.com/Wiradhika6051/Feminicare/ML/chatbot/dataset3.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 148087 (145K) [text/plain]\n",
            "Saving to: ‘dataset3.json’\n",
            "\n",
            "\rdataset3.json         0%[                    ]       0  --.-KB/s               \rdataset3.json       100%[===================>] 144.62K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-12-12 13:32:37 (6.86 MB/s) - ‘dataset3.json’ saved [148087/148087]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -o stopword_id.txt https://raw.githubusercontent.com/Wiradhika6051/Feminicare/ML/chatbot/stopword_id.txt"
      ],
      "metadata": {
        "id": "tMluX5TUGNvX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RyKVzxi7xsMh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "import re\n",
        "import json\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t4eD6q9xsMk"
      },
      "source": [
        "### load dataset dan preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Read stop words from file\n",
        "stop_words = set()\n",
        "with open('stopword_id.txt', 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        stop_words.add(line.strip())\n",
        "\n",
        "with open('dataset3.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "tags = []\n",
        "inputs = []\n",
        "responses = {}\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "\n",
        "for intent in data['intents']:\n",
        "    responses[intent['tag']] = intent['responses']\n",
        "    for line in intent['patterns']:\n",
        "        # Preprocess text\n",
        "        line = re.sub('[.,’\"\\'-?:!;]', '', line)\n",
        "\n",
        "        # Remove stopwords\n",
        "        line = remove_stopwords(line)\n",
        "\n",
        "        inputs.append(line)\n",
        "        tags.append(intent['tag'])\n",
        "\n",
        "# Create a DataFrame\n",
        "data_df = pd.DataFrame({\"inputs\": inputs, \"tags\": tags})\n"
      ],
      "metadata": {
        "id": "uIqDgpC_ZZsV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytba4TwcxsMl"
      },
      "source": [
        "### print dataset setelah preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P2E2AmpxsMm",
        "outputId": "d6da51c1-cdee-47b3-c5d2-842420743333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                inputs  \\\n",
            "0                                                hallo   \n",
            "1                                                  hai   \n",
            "2                                                 halo   \n",
            "3                                                  hei   \n",
            "4                                                   hi   \n",
            "..                                                 ...   \n",
            "716  Cara mencari dokter spesialis kesehatan menstr...   \n",
            "717  Pemilihan dokter yang cocok untuk masalah mens...   \n",
            "718  Bagaimana siklus menstruasi dapat berubah seir...   \n",
            "719  Perubahan dalam siklus menstruasi seiring bert...   \n",
            "720  Apakah siklus menstruasi dapat berubah saat me...   \n",
            "\n",
            "                                        tags  \n",
            "0                                      salam  \n",
            "1                                      salam  \n",
            "2                                      salam  \n",
            "3                                      salam  \n",
            "4                                      salam  \n",
            "..                                       ...  \n",
            "716           spesialis_kesehatan_menstruasi  \n",
            "717           spesialis_kesehatan_menstruasi  \n",
            "718  perubahan_siklus_menstruasi_dengan_usia  \n",
            "719  perubahan_siklus_menstruasi_dengan_usia  \n",
            "720  perubahan_siklus_menstruasi_dengan_usia  \n",
            "\n",
            "[721 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "print(data_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CxwHeNUuxsMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82fa155f-4c65-40c4-8e43-4a86f1622c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created successfully.\n"
          ]
        }
      ],
      "source": [
        "data_df.to_csv('output.csv', index=False)\n",
        "\n",
        "print(\"CSV file created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ye3m1KCMFIyh",
        "outputId": "392070c6-e862-4c20-8668-b375de41ef0d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  inputs   tags\n",
              "0  hallo  salam\n",
              "1    hai  salam\n",
              "2   halo  salam\n",
              "3    hei  salam\n",
              "4     hi  salam"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3dfb8010-b045-4f17-9f47-30e5c1c992df\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hallo</td>\n",
              "      <td>salam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hai</td>\n",
              "      <td>salam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>halo</td>\n",
              "      <td>salam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hei</td>\n",
              "      <td>salam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hi</td>\n",
              "      <td>salam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3dfb8010-b045-4f17-9f47-30e5c1c992df')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3dfb8010-b045-4f17-9f47-30e5c1c992df button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3dfb8010-b045-4f17-9f47-30e5c1c992df');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e69191d5-1fa2-49e2-94b9-0f2afa2e0179\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e69191d5-1fa2-49e2-94b9-0f2afa2e0179')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e69191d5-1fa2-49e2-94b9-0f2afa2e0179 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMCIuqgRxsMp"
      },
      "source": [
        "### preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hOD0jY_RxsMp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer(num_words=2000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(data_df['inputs'])\n",
        "\n",
        "# Encode the outputs\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(data_df['tags'])\n",
        "\n",
        "# Padding sequences\n",
        "sequences = tokenizer.texts_to_sequences(data_df['inputs'])\n",
        "max_len = max(len(sequence) for sequence in sequences)\n",
        "x_train = pad_sequences(sequences, maxlen=max_len, truncating='post')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azu5MKpTxsMq"
      },
      "source": [
        "### inisialisasi layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GlobalMaxPooling1D, Dropout, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(len(tokenizer.word_index)+1, 64, input_length=max_len),\n",
        "    LSTM(436, return_sequences=True),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dropout(0.5),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax'),\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "CDzxNc8R3g-8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GlobalMaxPooling1D, Dropout, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(len(tokenizer.word_index) + 1, 128, input_length=max_len),\n",
        "    LSTM(256, return_sequences=True),\n",
        "    LSTM(128),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dropout(0.2),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax'),\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "vOloBUz_LS_r",
        "outputId": "80e465f0-e475-40c2-8877-7f50ac564661"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Embedding, LSTM, GlobalMaxPooling1D, Dropout, Dense\\n\\nmodel = Sequential([\\n    Embedding(len(tokenizer.word_index) + 1, 128, input_length=max_len),\\n    LSTM(256, return_sequences=True),\\n    LSTM(128),\\n    GlobalMaxPooling1D(),\\n    Dropout(0.2),\\n    Dense(128, activation='relu'),\\n    Dropout(0.5),\\n    Dense(len(label_encoder.classes_), activation='softmax'),\\n])\\n\\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8o8PDsYxsMr"
      },
      "source": [
        "### training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oesE6R5uxsMr",
        "outputId": "474fc7ed-bf85-4e80-d169-cf0c79f8770c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 5.1497 - accuracy: 0.0052\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00690, saving model to best_model.h5\n",
            "18/18 [==============================] - 10s 240ms/step - loss: 5.1497 - accuracy: 0.0052 - val_loss: 5.1512 - val_accuracy: 0.0069\n",
            "Epoch 2/300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - ETA: 0s - loss: 5.1340 - accuracy: 0.0139\n",
            "Epoch 2: val_accuracy improved from 0.00690 to 0.02069, saving model to best_model.h5\n",
            "18/18 [==============================] - 3s 156ms/step - loss: 5.1340 - accuracy: 0.0139 - val_loss: 5.1602 - val_accuracy: 0.0207\n",
            "Epoch 3/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 5.1134 - accuracy: 0.0191\n",
            "Epoch 3: val_accuracy did not improve from 0.02069\n",
            "18/18 [==============================] - 3s 148ms/step - loss: 5.1134 - accuracy: 0.0191 - val_loss: 5.1808 - val_accuracy: 0.0207\n",
            "Epoch 4/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 5.0938 - accuracy: 0.0156\n",
            "Epoch 4: val_accuracy did not improve from 0.02069\n",
            "18/18 [==============================] - 3s 171ms/step - loss: 5.0938 - accuracy: 0.0156 - val_loss: 5.1970 - val_accuracy: 0.0207\n",
            "Epoch 5/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 5.0719 - accuracy: 0.0191\n",
            "Epoch 5: val_accuracy did not improve from 0.02069\n",
            "18/18 [==============================] - 4s 246ms/step - loss: 5.0719 - accuracy: 0.0191 - val_loss: 5.1812 - val_accuracy: 0.0207\n",
            "Epoch 6/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 5.0654 - accuracy: 0.0208\n",
            "Epoch 6: val_accuracy did not improve from 0.02069\n",
            "18/18 [==============================] - 2s 119ms/step - loss: 5.0654 - accuracy: 0.0208 - val_loss: 5.2788 - val_accuracy: 0.0207\n",
            "Epoch 7/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 5.0106 - accuracy: 0.0156\n",
            "Epoch 7: val_accuracy did not improve from 0.02069\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 5.0106 - accuracy: 0.0156 - val_loss: 5.2471 - val_accuracy: 0.0207\n",
            "Epoch 8/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 4.9808 - accuracy: 0.0174\n",
            "Epoch 8: val_accuracy did not improve from 0.02069\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 4.9808 - accuracy: 0.0174 - val_loss: 5.1127 - val_accuracy: 0.0207\n",
            "Epoch 9/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 4.8530 - accuracy: 0.0278\n",
            "Epoch 9: val_accuracy did not improve from 0.02069\n",
            "18/18 [==============================] - 1s 82ms/step - loss: 4.8530 - accuracy: 0.0278 - val_loss: 5.0671 - val_accuracy: 0.0207\n",
            "Epoch 10/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 4.7031 - accuracy: 0.0295\n",
            "Epoch 10: val_accuracy improved from 0.02069 to 0.02759, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 4.7031 - accuracy: 0.0295 - val_loss: 5.0247 - val_accuracy: 0.0276\n",
            "Epoch 11/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 4.6347 - accuracy: 0.0226\n",
            "Epoch 11: val_accuracy did not improve from 0.02759\n",
            "18/18 [==============================] - 1s 82ms/step - loss: 4.6347 - accuracy: 0.0226 - val_loss: 4.9771 - val_accuracy: 0.0276\n",
            "Epoch 12/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 4.5247 - accuracy: 0.0330\n",
            "Epoch 12: val_accuracy did not improve from 0.02759\n",
            "18/18 [==============================] - 2s 121ms/step - loss: 4.5247 - accuracy: 0.0330 - val_loss: 4.9293 - val_accuracy: 0.0276\n",
            "Epoch 13/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 4.4504 - accuracy: 0.0347\n",
            "Epoch 13: val_accuracy improved from 0.02759 to 0.03448, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 121ms/step - loss: 4.4504 - accuracy: 0.0347 - val_loss: 4.9155 - val_accuracy: 0.0345\n",
            "Epoch 14/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 4.3725 - accuracy: 0.0399\n",
            "Epoch 14: val_accuracy did not improve from 0.03448\n",
            "18/18 [==============================] - 2s 106ms/step - loss: 4.3725 - accuracy: 0.0399 - val_loss: 4.8182 - val_accuracy: 0.0207\n",
            "Epoch 15/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 4.3190 - accuracy: 0.0365\n",
            "Epoch 15: val_accuracy did not improve from 0.03448\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 4.3190 - accuracy: 0.0365 - val_loss: 4.7764 - val_accuracy: 0.0276\n",
            "Epoch 16/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 4.1993 - accuracy: 0.0451\n",
            "Epoch 16: val_accuracy did not improve from 0.03448\n",
            "18/18 [==============================] - 1s 82ms/step - loss: 4.1993 - accuracy: 0.0451 - val_loss: 4.8357 - val_accuracy: 0.0138\n",
            "Epoch 17/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 4.1439 - accuracy: 0.0538\n",
            "Epoch 17: val_accuracy improved from 0.03448 to 0.06207, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 4.1439 - accuracy: 0.0538 - val_loss: 4.7315 - val_accuracy: 0.0621\n",
            "Epoch 18/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 4.0054 - accuracy: 0.0694\n",
            "Epoch 18: val_accuracy did not improve from 0.06207\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 4.0054 - accuracy: 0.0694 - val_loss: 4.6352 - val_accuracy: 0.0483\n",
            "Epoch 19/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 3.9188 - accuracy: 0.0747\n",
            "Epoch 19: val_accuracy did not improve from 0.06207\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 3.9188 - accuracy: 0.0747 - val_loss: 4.5772 - val_accuracy: 0.0483\n",
            "Epoch 20/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 3.8368 - accuracy: 0.0816\n",
            "Epoch 20: val_accuracy improved from 0.06207 to 0.08276, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 3.8368 - accuracy: 0.0816 - val_loss: 4.4782 - val_accuracy: 0.0828\n",
            "Epoch 21/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 3.7302 - accuracy: 0.1007\n",
            "Epoch 21: val_accuracy did not improve from 0.08276\n",
            "18/18 [==============================] - 2s 120ms/step - loss: 3.7302 - accuracy: 0.1007 - val_loss: 4.3512 - val_accuracy: 0.0759\n",
            "Epoch 22/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 3.5753 - accuracy: 0.1128\n",
            "Epoch 22: val_accuracy did not improve from 0.08276\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 3.5753 - accuracy: 0.1128 - val_loss: 4.3010 - val_accuracy: 0.0690\n",
            "Epoch 23/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 3.4489 - accuracy: 0.1319\n",
            "Epoch 23: val_accuracy improved from 0.08276 to 0.11034, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 109ms/step - loss: 3.4489 - accuracy: 0.1319 - val_loss: 4.1465 - val_accuracy: 0.1103\n",
            "Epoch 24/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 3.2282 - accuracy: 0.1649\n",
            "Epoch 24: val_accuracy did not improve from 0.11034\n",
            "18/18 [==============================] - 1s 82ms/step - loss: 3.2282 - accuracy: 0.1649 - val_loss: 4.1012 - val_accuracy: 0.0828\n",
            "Epoch 25/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 3.1549 - accuracy: 0.1528\n",
            "Epoch 25: val_accuracy improved from 0.11034 to 0.14483, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 3.1549 - accuracy: 0.1528 - val_loss: 3.9076 - val_accuracy: 0.1448\n",
            "Epoch 26/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 2.9721 - accuracy: 0.2049\n",
            "Epoch 26: val_accuracy improved from 0.14483 to 0.16552, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 2.9721 - accuracy: 0.2049 - val_loss: 3.8045 - val_accuracy: 0.1655\n",
            "Epoch 27/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 2.7986 - accuracy: 0.2413\n",
            "Epoch 27: val_accuracy did not improve from 0.16552\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 2.7986 - accuracy: 0.2413 - val_loss: 3.7465 - val_accuracy: 0.1655\n",
            "Epoch 28/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 2.6824 - accuracy: 0.2378\n",
            "Epoch 28: val_accuracy improved from 0.16552 to 0.17241, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 2.6824 - accuracy: 0.2378 - val_loss: 3.5185 - val_accuracy: 0.1724\n",
            "Epoch 29/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 2.5063 - accuracy: 0.2865\n",
            "Epoch 29: val_accuracy improved from 0.17241 to 0.22759, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 2.5063 - accuracy: 0.2865 - val_loss: 3.3790 - val_accuracy: 0.2276\n",
            "Epoch 30/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 2.3831 - accuracy: 0.3438\n",
            "Epoch 30: val_accuracy did not improve from 0.22759\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 2.3831 - accuracy: 0.3438 - val_loss: 3.3063 - val_accuracy: 0.1724\n",
            "Epoch 31/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 2.3058 - accuracy: 0.3628\n",
            "Epoch 31: val_accuracy improved from 0.22759 to 0.28276, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 2.3058 - accuracy: 0.3628 - val_loss: 3.1556 - val_accuracy: 0.2828\n",
            "Epoch 32/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 2.1894 - accuracy: 0.3698\n",
            "Epoch 32: val_accuracy did not improve from 0.28276\n",
            "18/18 [==============================] - 2s 97ms/step - loss: 2.1894 - accuracy: 0.3698 - val_loss: 3.0389 - val_accuracy: 0.2207\n",
            "Epoch 33/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 2.0160 - accuracy: 0.3924\n",
            "Epoch 33: val_accuracy did not improve from 0.28276\n",
            "18/18 [==============================] - 1s 80ms/step - loss: 2.0160 - accuracy: 0.3924 - val_loss: 3.0491 - val_accuracy: 0.2621\n",
            "Epoch 34/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.9769 - accuracy: 0.4392\n",
            "Epoch 34: val_accuracy did not improve from 0.28276\n",
            "18/18 [==============================] - 1s 80ms/step - loss: 1.9769 - accuracy: 0.4392 - val_loss: 2.9065 - val_accuracy: 0.2483\n",
            "Epoch 35/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.7099 - accuracy: 0.4861\n",
            "Epoch 35: val_accuracy improved from 0.28276 to 0.30345, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 1.7099 - accuracy: 0.4861 - val_loss: 2.8815 - val_accuracy: 0.3034\n",
            "Epoch 36/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.6668 - accuracy: 0.4913\n",
            "Epoch 36: val_accuracy did not improve from 0.30345\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 1.6668 - accuracy: 0.4913 - val_loss: 2.8117 - val_accuracy: 0.2897\n",
            "Epoch 37/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.5450 - accuracy: 0.5469\n",
            "Epoch 37: val_accuracy improved from 0.30345 to 0.32414, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 1.5450 - accuracy: 0.5469 - val_loss: 2.6997 - val_accuracy: 0.3241\n",
            "Epoch 38/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.5283 - accuracy: 0.5399\n",
            "Epoch 38: val_accuracy improved from 0.32414 to 0.35172, saving model to best_model.h5\n",
            "18/18 [==============================] - 1s 81ms/step - loss: 1.5283 - accuracy: 0.5399 - val_loss: 2.6295 - val_accuracy: 0.3517\n",
            "Epoch 39/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.4524 - accuracy: 0.5382\n",
            "Epoch 39: val_accuracy did not improve from 0.35172\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 1.4524 - accuracy: 0.5382 - val_loss: 2.5405 - val_accuracy: 0.3172\n",
            "Epoch 40/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.4212 - accuracy: 0.5833\n",
            "Epoch 40: val_accuracy improved from 0.35172 to 0.37931, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 120ms/step - loss: 1.4212 - accuracy: 0.5833 - val_loss: 2.5340 - val_accuracy: 0.3793\n",
            "Epoch 41/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.2779 - accuracy: 0.6163\n",
            "Epoch 41: val_accuracy did not improve from 0.37931\n",
            "18/18 [==============================] - 2s 100ms/step - loss: 1.2779 - accuracy: 0.6163 - val_loss: 2.4994 - val_accuracy: 0.3724\n",
            "Epoch 42/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.2117 - accuracy: 0.6302\n",
            "Epoch 42: val_accuracy did not improve from 0.37931\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 1.2117 - accuracy: 0.6302 - val_loss: 2.4839 - val_accuracy: 0.3655\n",
            "Epoch 43/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.1084 - accuracy: 0.6840\n",
            "Epoch 43: val_accuracy did not improve from 0.37931\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 1.1084 - accuracy: 0.6840 - val_loss: 2.4478 - val_accuracy: 0.3517\n",
            "Epoch 44/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.1064 - accuracy: 0.6719\n",
            "Epoch 44: val_accuracy did not improve from 0.37931\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 1.1064 - accuracy: 0.6719 - val_loss: 2.4653 - val_accuracy: 0.3655\n",
            "Epoch 45/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 1.0075 - accuracy: 0.6892\n",
            "Epoch 45: val_accuracy improved from 0.37931 to 0.41379, saving model to best_model.h5\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 1.0075 - accuracy: 0.6892 - val_loss: 2.3468 - val_accuracy: 0.4138\n",
            "Epoch 46/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.9931 - accuracy: 0.6962\n",
            "Epoch 46: val_accuracy did not improve from 0.41379\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.9931 - accuracy: 0.6962 - val_loss: 2.4750 - val_accuracy: 0.4138\n",
            "Epoch 47/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.8924 - accuracy: 0.7135\n",
            "Epoch 47: val_accuracy improved from 0.41379 to 0.46207, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.8924 - accuracy: 0.7135 - val_loss: 2.2250 - val_accuracy: 0.4621\n",
            "Epoch 48/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.9021 - accuracy: 0.6823\n",
            "Epoch 48: val_accuracy did not improve from 0.46207\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.9021 - accuracy: 0.6823 - val_loss: 2.4203 - val_accuracy: 0.4552\n",
            "Epoch 49/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.8149 - accuracy: 0.7604\n",
            "Epoch 49: val_accuracy did not improve from 0.46207\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.8149 - accuracy: 0.7604 - val_loss: 2.3081 - val_accuracy: 0.4483\n",
            "Epoch 50/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.7527 - accuracy: 0.7708\n",
            "Epoch 50: val_accuracy improved from 0.46207 to 0.47586, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 101ms/step - loss: 0.7527 - accuracy: 0.7708 - val_loss: 2.3797 - val_accuracy: 0.4759\n",
            "Epoch 51/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.7325 - accuracy: 0.7778\n",
            "Epoch 51: val_accuracy did not improve from 0.47586\n",
            "18/18 [==============================] - 1s 82ms/step - loss: 0.7325 - accuracy: 0.7778 - val_loss: 2.2630 - val_accuracy: 0.4414\n",
            "Epoch 52/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.7567 - accuracy: 0.7604\n",
            "Epoch 52: val_accuracy did not improve from 0.47586\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 0.7567 - accuracy: 0.7604 - val_loss: 2.2533 - val_accuracy: 0.4690\n",
            "Epoch 53/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.6730 - accuracy: 0.7917\n",
            "Epoch 53: val_accuracy improved from 0.47586 to 0.48966, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.6730 - accuracy: 0.7917 - val_loss: 2.2332 - val_accuracy: 0.4897\n",
            "Epoch 54/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.6022 - accuracy: 0.8142\n",
            "Epoch 54: val_accuracy did not improve from 0.48966\n",
            "18/18 [==============================] - 1s 84ms/step - loss: 0.6022 - accuracy: 0.8142 - val_loss: 2.4383 - val_accuracy: 0.4345\n",
            "Epoch 55/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.6490 - accuracy: 0.7917\n",
            "Epoch 55: val_accuracy did not improve from 0.48966\n",
            "18/18 [==============================] - 1s 82ms/step - loss: 0.6490 - accuracy: 0.7917 - val_loss: 2.2259 - val_accuracy: 0.4897\n",
            "Epoch 56/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.6501 - accuracy: 0.8177\n",
            "Epoch 56: val_accuracy did not improve from 0.48966\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 0.6501 - accuracy: 0.8177 - val_loss: 2.2913 - val_accuracy: 0.4828\n",
            "Epoch 57/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.5671 - accuracy: 0.8299\n",
            "Epoch 57: val_accuracy improved from 0.48966 to 0.49655, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 126ms/step - loss: 0.5671 - accuracy: 0.8299 - val_loss: 2.3334 - val_accuracy: 0.4966\n",
            "Epoch 58/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.5395 - accuracy: 0.8420\n",
            "Epoch 58: val_accuracy did not improve from 0.49655\n",
            "18/18 [==============================] - 2s 119ms/step - loss: 0.5395 - accuracy: 0.8420 - val_loss: 2.2987 - val_accuracy: 0.4966\n",
            "Epoch 59/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.5105 - accuracy: 0.8455\n",
            "Epoch 59: val_accuracy did not improve from 0.49655\n",
            "18/18 [==============================] - 2s 94ms/step - loss: 0.5105 - accuracy: 0.8455 - val_loss: 2.4740 - val_accuracy: 0.4828\n",
            "Epoch 60/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.4938 - accuracy: 0.8663\n",
            "Epoch 60: val_accuracy did not improve from 0.49655\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.4938 - accuracy: 0.8663 - val_loss: 2.3015 - val_accuracy: 0.4966\n",
            "Epoch 61/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.4715 - accuracy: 0.8490\n",
            "Epoch 61: val_accuracy improved from 0.49655 to 0.51034, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.4715 - accuracy: 0.8490 - val_loss: 2.1816 - val_accuracy: 0.5103\n",
            "Epoch 62/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.4390 - accuracy: 0.8715\n",
            "Epoch 62: val_accuracy did not improve from 0.51034\n",
            "18/18 [==============================] - 1s 81ms/step - loss: 0.4390 - accuracy: 0.8715 - val_loss: 2.3712 - val_accuracy: 0.5034\n",
            "Epoch 63/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.4220 - accuracy: 0.8733\n",
            "Epoch 63: val_accuracy improved from 0.51034 to 0.52414, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.4220 - accuracy: 0.8733 - val_loss: 2.1529 - val_accuracy: 0.5241\n",
            "Epoch 64/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.3980 - accuracy: 0.8767\n",
            "Epoch 64: val_accuracy improved from 0.52414 to 0.53103, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 100ms/step - loss: 0.3980 - accuracy: 0.8767 - val_loss: 2.3075 - val_accuracy: 0.5310\n",
            "Epoch 65/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.4188 - accuracy: 0.8733\n",
            "Epoch 65: val_accuracy did not improve from 0.53103\n",
            "18/18 [==============================] - 2s 94ms/step - loss: 0.4188 - accuracy: 0.8733 - val_loss: 2.3819 - val_accuracy: 0.5172\n",
            "Epoch 66/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.4296 - accuracy: 0.8663\n",
            "Epoch 66: val_accuracy improved from 0.53103 to 0.54483, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 130ms/step - loss: 0.4296 - accuracy: 0.8663 - val_loss: 2.1728 - val_accuracy: 0.5448\n",
            "Epoch 67/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.3694 - accuracy: 0.8941\n",
            "Epoch 67: val_accuracy did not improve from 0.54483\n",
            "18/18 [==============================] - 2s 121ms/step - loss: 0.3694 - accuracy: 0.8941 - val_loss: 2.1803 - val_accuracy: 0.5448\n",
            "Epoch 68/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.3737 - accuracy: 0.8802\n",
            "Epoch 68: val_accuracy improved from 0.54483 to 0.55172, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.3737 - accuracy: 0.8802 - val_loss: 2.2207 - val_accuracy: 0.5517\n",
            "Epoch 69/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.8993\n",
            "Epoch 69: val_accuracy did not improve from 0.55172\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.3323 - accuracy: 0.8993 - val_loss: 2.4750 - val_accuracy: 0.5034\n",
            "Epoch 70/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.3346 - accuracy: 0.8993\n",
            "Epoch 70: val_accuracy did not improve from 0.55172\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.3346 - accuracy: 0.8993 - val_loss: 2.2576 - val_accuracy: 0.5310\n",
            "Epoch 71/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.3281 - accuracy: 0.9010\n",
            "Epoch 71: val_accuracy improved from 0.55172 to 0.56552, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 125ms/step - loss: 0.3281 - accuracy: 0.9010 - val_loss: 2.1723 - val_accuracy: 0.5655\n",
            "Epoch 72/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.9132\n",
            "Epoch 72: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 2s 131ms/step - loss: 0.3097 - accuracy: 0.9132 - val_loss: 2.3355 - val_accuracy: 0.5310\n",
            "Epoch 73/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.3211 - accuracy: 0.8924\n",
            "Epoch 73: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 3s 154ms/step - loss: 0.3211 - accuracy: 0.8924 - val_loss: 2.2003 - val_accuracy: 0.5241\n",
            "Epoch 74/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2979 - accuracy: 0.9097\n",
            "Epoch 74: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 2s 120ms/step - loss: 0.2979 - accuracy: 0.9097 - val_loss: 2.2823 - val_accuracy: 0.5586\n",
            "Epoch 75/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.8906\n",
            "Epoch 75: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 2s 97ms/step - loss: 0.2971 - accuracy: 0.8906 - val_loss: 2.2458 - val_accuracy: 0.5034\n",
            "Epoch 76/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2737 - accuracy: 0.9097\n",
            "Epoch 76: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.2737 - accuracy: 0.9097 - val_loss: 2.2621 - val_accuracy: 0.5448\n",
            "Epoch 77/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.9201\n",
            "Epoch 77: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.2479 - accuracy: 0.9201 - val_loss: 2.3242 - val_accuracy: 0.5448\n",
            "Epoch 78/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2417 - accuracy: 0.9288\n",
            "Epoch 78: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.2417 - accuracy: 0.9288 - val_loss: 2.2639 - val_accuracy: 0.5517\n",
            "Epoch 79/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.9358\n",
            "Epoch 79: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 1s 81ms/step - loss: 0.2378 - accuracy: 0.9358 - val_loss: 2.3777 - val_accuracy: 0.5310\n",
            "Epoch 80/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1847 - accuracy: 0.9427\n",
            "Epoch 80: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 0.1847 - accuracy: 0.9427 - val_loss: 2.3724 - val_accuracy: 0.5241\n",
            "Epoch 81/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2014 - accuracy: 0.9410\n",
            "Epoch 81: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.2014 - accuracy: 0.9410 - val_loss: 2.1890 - val_accuracy: 0.5586\n",
            "Epoch 82/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9288\n",
            "Epoch 82: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 2s 122ms/step - loss: 0.2576 - accuracy: 0.9288 - val_loss: 2.4467 - val_accuracy: 0.5310\n",
            "Epoch 83/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.9201\n",
            "Epoch 83: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.2381 - accuracy: 0.9201 - val_loss: 2.3517 - val_accuracy: 0.5448\n",
            "Epoch 84/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9306\n",
            "Epoch 84: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 2s 97ms/step - loss: 0.2440 - accuracy: 0.9306 - val_loss: 2.4262 - val_accuracy: 0.5586\n",
            "Epoch 85/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2004 - accuracy: 0.9410\n",
            "Epoch 85: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.2004 - accuracy: 0.9410 - val_loss: 2.2427 - val_accuracy: 0.5241\n",
            "Epoch 86/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1758 - accuracy: 0.9549\n",
            "Epoch 86: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 0.1758 - accuracy: 0.9549 - val_loss: 2.3912 - val_accuracy: 0.5655\n",
            "Epoch 87/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.9184\n",
            "Epoch 87: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.2321 - accuracy: 0.9184 - val_loss: 2.4272 - val_accuracy: 0.5310\n",
            "Epoch 88/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2103 - accuracy: 0.9444\n",
            "Epoch 88: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.2103 - accuracy: 0.9444 - val_loss: 2.3929 - val_accuracy: 0.5586\n",
            "Epoch 89/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1793 - accuracy: 0.9444\n",
            "Epoch 89: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 0.1793 - accuracy: 0.9444 - val_loss: 2.1602 - val_accuracy: 0.5655\n",
            "Epoch 90/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1676 - accuracy: 0.9497\n",
            "Epoch 90: val_accuracy did not improve from 0.56552\n",
            "18/18 [==============================] - 2s 97ms/step - loss: 0.1676 - accuracy: 0.9497 - val_loss: 2.5091 - val_accuracy: 0.5517\n",
            "Epoch 91/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9462\n",
            "Epoch 91: val_accuracy improved from 0.56552 to 0.57241, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 123ms/step - loss: 0.1684 - accuracy: 0.9462 - val_loss: 2.3147 - val_accuracy: 0.5724\n",
            "Epoch 92/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9549\n",
            "Epoch 92: val_accuracy did not improve from 0.57241\n",
            "18/18 [==============================] - 2s 127ms/step - loss: 0.1586 - accuracy: 0.9549 - val_loss: 2.2257 - val_accuracy: 0.5517\n",
            "Epoch 93/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9531\n",
            "Epoch 93: val_accuracy did not improve from 0.57241\n",
            "18/18 [==============================] - 1s 81ms/step - loss: 0.1602 - accuracy: 0.9531 - val_loss: 2.3261 - val_accuracy: 0.5448\n",
            "Epoch 94/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1475 - accuracy: 0.9566\n",
            "Epoch 94: val_accuracy did not improve from 0.57241\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.1475 - accuracy: 0.9566 - val_loss: 2.2269 - val_accuracy: 0.5724\n",
            "Epoch 95/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9635\n",
            "Epoch 95: val_accuracy did not improve from 0.57241\n",
            "18/18 [==============================] - 1s 82ms/step - loss: 0.1153 - accuracy: 0.9635 - val_loss: 2.3886 - val_accuracy: 0.5655\n",
            "Epoch 96/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1311 - accuracy: 0.9670\n",
            "Epoch 96: val_accuracy improved from 0.57241 to 0.57931, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.1311 - accuracy: 0.9670 - val_loss: 2.4038 - val_accuracy: 0.5793\n",
            "Epoch 97/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1538 - accuracy: 0.9583\n",
            "Epoch 97: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 1s 81ms/step - loss: 0.1538 - accuracy: 0.9583 - val_loss: 2.5638 - val_accuracy: 0.5586\n",
            "Epoch 98/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1428 - accuracy: 0.9462\n",
            "Epoch 98: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 1s 81ms/step - loss: 0.1428 - accuracy: 0.9462 - val_loss: 2.9782 - val_accuracy: 0.5310\n",
            "Epoch 99/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1448 - accuracy: 0.9635\n",
            "Epoch 99: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 2s 96ms/step - loss: 0.1448 - accuracy: 0.9635 - val_loss: 2.6263 - val_accuracy: 0.5517\n",
            "Epoch 100/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9618\n",
            "Epoch 100: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 2s 125ms/step - loss: 0.1392 - accuracy: 0.9618 - val_loss: 2.6929 - val_accuracy: 0.5655\n",
            "Epoch 101/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9549\n",
            "Epoch 101: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 2s 121ms/step - loss: 0.1615 - accuracy: 0.9549 - val_loss: 2.4564 - val_accuracy: 0.5586\n",
            "Epoch 102/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9566\n",
            "Epoch 102: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 0.1365 - accuracy: 0.9566 - val_loss: 2.5127 - val_accuracy: 0.5586\n",
            "Epoch 103/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1191 - accuracy: 0.9705\n",
            "Epoch 103: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 1s 81ms/step - loss: 0.1191 - accuracy: 0.9705 - val_loss: 2.4554 - val_accuracy: 0.5586\n",
            "Epoch 104/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1482 - accuracy: 0.9549\n",
            "Epoch 104: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 1s 82ms/step - loss: 0.1482 - accuracy: 0.9549 - val_loss: 2.4437 - val_accuracy: 0.5724\n",
            "Epoch 105/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1543 - accuracy: 0.9427\n",
            "Epoch 105: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 1s 82ms/step - loss: 0.1543 - accuracy: 0.9427 - val_loss: 2.4841 - val_accuracy: 0.5793\n",
            "Epoch 106/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9618\n",
            "Epoch 106: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.1232 - accuracy: 0.9618 - val_loss: 2.3461 - val_accuracy: 0.5724\n",
            "Epoch 107/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.9635\n",
            "Epoch 107: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 0.1322 - accuracy: 0.9635 - val_loss: 2.4365 - val_accuracy: 0.5724\n",
            "Epoch 108/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1297 - accuracy: 0.9583\n",
            "Epoch 108: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 2s 96ms/step - loss: 0.1297 - accuracy: 0.9583 - val_loss: 2.5841 - val_accuracy: 0.5586\n",
            "Epoch 109/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1509 - accuracy: 0.9531\n",
            "Epoch 109: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 2s 127ms/step - loss: 0.1509 - accuracy: 0.9531 - val_loss: 2.5363 - val_accuracy: 0.5448\n",
            "Epoch 110/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9740\n",
            "Epoch 110: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0934 - accuracy: 0.9740 - val_loss: 2.7397 - val_accuracy: 0.5448\n",
            "Epoch 111/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9653\n",
            "Epoch 111: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.0906 - accuracy: 0.9653 - val_loss: 2.6022 - val_accuracy: 0.5310\n",
            "Epoch 112/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9740\n",
            "Epoch 112: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0911 - accuracy: 0.9740 - val_loss: 2.8901 - val_accuracy: 0.5310\n",
            "Epoch 113/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9688\n",
            "Epoch 113: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 0.1187 - accuracy: 0.9688 - val_loss: 2.5836 - val_accuracy: 0.5448\n",
            "Epoch 114/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 0.9635\n",
            "Epoch 114: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.1048 - accuracy: 0.9635 - val_loss: 2.5068 - val_accuracy: 0.5793\n",
            "Epoch 115/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9653\n",
            "Epoch 115: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 1s 82ms/step - loss: 0.1293 - accuracy: 0.9653 - val_loss: 2.3347 - val_accuracy: 0.5655\n",
            "Epoch 116/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 0.9566\n",
            "Epoch 116: val_accuracy did not improve from 0.57931\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 0.1353 - accuracy: 0.9566 - val_loss: 2.3614 - val_accuracy: 0.5793\n",
            "Epoch 117/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.9583\n",
            "Epoch 117: val_accuracy improved from 0.57931 to 0.60690, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 98ms/step - loss: 0.1342 - accuracy: 0.9583 - val_loss: 2.3246 - val_accuracy: 0.6069\n",
            "Epoch 118/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9618\n",
            "Epoch 118: val_accuracy did not improve from 0.60690\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1309 - accuracy: 0.9618 - val_loss: 2.3858 - val_accuracy: 0.5862\n",
            "Epoch 119/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9653\n",
            "Epoch 119: val_accuracy improved from 0.60690 to 0.62069, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 121ms/step - loss: 0.1101 - accuracy: 0.9653 - val_loss: 2.5919 - val_accuracy: 0.6207\n",
            "Epoch 120/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9740\n",
            "Epoch 120: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 89ms/step - loss: 0.1019 - accuracy: 0.9740 - val_loss: 2.5873 - val_accuracy: 0.6000\n",
            "Epoch 121/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.9601\n",
            "Epoch 121: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.1172 - accuracy: 0.9601 - val_loss: 2.3017 - val_accuracy: 0.5862\n",
            "Epoch 122/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1020 - accuracy: 0.9740\n",
            "Epoch 122: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.1020 - accuracy: 0.9740 - val_loss: 2.6694 - val_accuracy: 0.5862\n",
            "Epoch 123/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9635\n",
            "Epoch 123: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 81ms/step - loss: 0.1011 - accuracy: 0.9635 - val_loss: 2.4358 - val_accuracy: 0.5793\n",
            "Epoch 124/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0941 - accuracy: 0.9670\n",
            "Epoch 124: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 0.0941 - accuracy: 0.9670 - val_loss: 2.4251 - val_accuracy: 0.5793\n",
            "Epoch 125/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1082 - accuracy: 0.9705\n",
            "Epoch 125: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 81ms/step - loss: 0.1082 - accuracy: 0.9705 - val_loss: 2.6181 - val_accuracy: 0.5655\n",
            "Epoch 126/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9792\n",
            "Epoch 126: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 106ms/step - loss: 0.0783 - accuracy: 0.9792 - val_loss: 2.3573 - val_accuracy: 0.5931\n",
            "Epoch 127/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0966 - accuracy: 0.9670\n",
            "Epoch 127: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0966 - accuracy: 0.9670 - val_loss: 2.3337 - val_accuracy: 0.6000\n",
            "Epoch 128/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9757\n",
            "Epoch 128: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.0683 - accuracy: 0.9757 - val_loss: 2.4973 - val_accuracy: 0.6069\n",
            "Epoch 129/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1053 - accuracy: 0.9618\n",
            "Epoch 129: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 79ms/step - loss: 0.1053 - accuracy: 0.9618 - val_loss: 2.5479 - val_accuracy: 0.5931\n",
            "Epoch 130/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9653\n",
            "Epoch 130: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 0.1099 - accuracy: 0.9653 - val_loss: 2.3186 - val_accuracy: 0.6138\n",
            "Epoch 131/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9601\n",
            "Epoch 131: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 84ms/step - loss: 0.1216 - accuracy: 0.9601 - val_loss: 2.5542 - val_accuracy: 0.5724\n",
            "Epoch 132/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9826\n",
            "Epoch 132: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 81ms/step - loss: 0.0734 - accuracy: 0.9826 - val_loss: 2.7151 - val_accuracy: 0.5862\n",
            "Epoch 133/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1025 - accuracy: 0.9705\n",
            "Epoch 133: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 81ms/step - loss: 0.1025 - accuracy: 0.9705 - val_loss: 2.4794 - val_accuracy: 0.6000\n",
            "Epoch 134/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9740\n",
            "Epoch 134: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0810 - accuracy: 0.9740 - val_loss: 2.4681 - val_accuracy: 0.5931\n",
            "Epoch 135/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9688\n",
            "Epoch 135: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 96ms/step - loss: 0.1114 - accuracy: 0.9688 - val_loss: 2.9212 - val_accuracy: 0.5586\n",
            "Epoch 136/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9809\n",
            "Epoch 136: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 133ms/step - loss: 0.0834 - accuracy: 0.9809 - val_loss: 2.7914 - val_accuracy: 0.5517\n",
            "Epoch 137/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1069 - accuracy: 0.9618\n",
            "Epoch 137: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.1069 - accuracy: 0.9618 - val_loss: 2.7069 - val_accuracy: 0.5724\n",
            "Epoch 138/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9601\n",
            "Epoch 138: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.1158 - accuracy: 0.9601 - val_loss: 2.8151 - val_accuracy: 0.5586\n",
            "Epoch 139/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9670\n",
            "Epoch 139: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 82ms/step - loss: 0.1208 - accuracy: 0.9670 - val_loss: 2.6652 - val_accuracy: 0.5586\n",
            "Epoch 140/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9722\n",
            "Epoch 140: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 82ms/step - loss: 0.0915 - accuracy: 0.9722 - val_loss: 2.5866 - val_accuracy: 0.5724\n",
            "Epoch 141/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0912 - accuracy: 0.9705\n",
            "Epoch 141: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 84ms/step - loss: 0.0912 - accuracy: 0.9705 - val_loss: 2.5845 - val_accuracy: 0.5724\n",
            "Epoch 142/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1105 - accuracy: 0.9653\n",
            "Epoch 142: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.1105 - accuracy: 0.9653 - val_loss: 2.4808 - val_accuracy: 0.6000\n",
            "Epoch 143/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9653\n",
            "Epoch 143: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 89ms/step - loss: 0.1045 - accuracy: 0.9653 - val_loss: 2.4844 - val_accuracy: 0.5724\n",
            "Epoch 144/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9740\n",
            "Epoch 144: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 125ms/step - loss: 0.0738 - accuracy: 0.9740 - val_loss: 2.6597 - val_accuracy: 0.5862\n",
            "Epoch 145/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9740\n",
            "Epoch 145: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 131ms/step - loss: 0.0802 - accuracy: 0.9740 - val_loss: 2.6933 - val_accuracy: 0.5931\n",
            "Epoch 146/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9722\n",
            "Epoch 146: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.0919 - accuracy: 0.9722 - val_loss: 2.5658 - val_accuracy: 0.5724\n",
            "Epoch 147/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9722\n",
            "Epoch 147: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.0935 - accuracy: 0.9722 - val_loss: 2.5031 - val_accuracy: 0.6000\n",
            "Epoch 148/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9792\n",
            "Epoch 148: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 89ms/step - loss: 0.0529 - accuracy: 0.9792 - val_loss: 2.5465 - val_accuracy: 0.5655\n",
            "Epoch 149/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9826\n",
            "Epoch 149: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0624 - accuracy: 0.9826 - val_loss: 2.7540 - val_accuracy: 0.5793\n",
            "Epoch 150/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9740\n",
            "Epoch 150: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0809 - accuracy: 0.9740 - val_loss: 2.4448 - val_accuracy: 0.5931\n",
            "Epoch 151/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9688\n",
            "Epoch 151: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 0.0911 - accuracy: 0.9688 - val_loss: 2.9117 - val_accuracy: 0.5862\n",
            "Epoch 152/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9635\n",
            "Epoch 152: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 0.0918 - accuracy: 0.9635 - val_loss: 2.5502 - val_accuracy: 0.6069\n",
            "Epoch 153/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9705\n",
            "Epoch 153: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0989 - accuracy: 0.9705 - val_loss: 2.3273 - val_accuracy: 0.5862\n",
            "Epoch 154/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9826\n",
            "Epoch 154: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0734 - accuracy: 0.9826 - val_loss: 2.5614 - val_accuracy: 0.5862\n",
            "Epoch 155/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9740\n",
            "Epoch 155: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 108ms/step - loss: 0.0944 - accuracy: 0.9740 - val_loss: 2.5685 - val_accuracy: 0.5931\n",
            "Epoch 156/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9722\n",
            "Epoch 156: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0770 - accuracy: 0.9722 - val_loss: 2.5775 - val_accuracy: 0.6069\n",
            "Epoch 157/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9705\n",
            "Epoch 157: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0875 - accuracy: 0.9705 - val_loss: 2.6376 - val_accuracy: 0.6000\n",
            "Epoch 158/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9722\n",
            "Epoch 158: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0804 - accuracy: 0.9722 - val_loss: 2.7596 - val_accuracy: 0.5793\n",
            "Epoch 159/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9705\n",
            "Epoch 159: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0591 - accuracy: 0.9705 - val_loss: 2.6657 - val_accuracy: 0.5793\n",
            "Epoch 160/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9757\n",
            "Epoch 160: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0708 - accuracy: 0.9757 - val_loss: 2.6404 - val_accuracy: 0.5724\n",
            "Epoch 161/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0633 - accuracy: 0.9809\n",
            "Epoch 161: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0633 - accuracy: 0.9809 - val_loss: 2.7478 - val_accuracy: 0.5793\n",
            "Epoch 162/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9774\n",
            "Epoch 162: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 123ms/step - loss: 0.0776 - accuracy: 0.9774 - val_loss: 2.6208 - val_accuracy: 0.5931\n",
            "Epoch 163/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9792\n",
            "Epoch 163: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 122ms/step - loss: 0.0618 - accuracy: 0.9792 - val_loss: 2.7396 - val_accuracy: 0.5793\n",
            "Epoch 164/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9826\n",
            "Epoch 164: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 100ms/step - loss: 0.0674 - accuracy: 0.9826 - val_loss: 2.7195 - val_accuracy: 0.5931\n",
            "Epoch 165/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9861\n",
            "Epoch 165: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.0451 - accuracy: 0.9861 - val_loss: 2.6559 - val_accuracy: 0.6000\n",
            "Epoch 166/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9809\n",
            "Epoch 166: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0646 - accuracy: 0.9809 - val_loss: 2.4919 - val_accuracy: 0.6000\n",
            "Epoch 167/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9844\n",
            "Epoch 167: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.0485 - accuracy: 0.9844 - val_loss: 2.4471 - val_accuracy: 0.5931\n",
            "Epoch 168/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9688\n",
            "Epoch 168: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 99ms/step - loss: 0.0819 - accuracy: 0.9688 - val_loss: 2.6313 - val_accuracy: 0.5724\n",
            "Epoch 169/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9705\n",
            "Epoch 169: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.0687 - accuracy: 0.9705 - val_loss: 2.4690 - val_accuracy: 0.6069\n",
            "Epoch 170/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9740\n",
            "Epoch 170: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 93ms/step - loss: 0.0741 - accuracy: 0.9740 - val_loss: 2.7995 - val_accuracy: 0.5931\n",
            "Epoch 171/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9878\n",
            "Epoch 171: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.0558 - accuracy: 0.9878 - val_loss: 2.6007 - val_accuracy: 0.5862\n",
            "Epoch 172/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9757\n",
            "Epoch 172: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 120ms/step - loss: 0.0798 - accuracy: 0.9757 - val_loss: 2.6054 - val_accuracy: 0.5586\n",
            "Epoch 173/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9722\n",
            "Epoch 173: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 94ms/step - loss: 0.0757 - accuracy: 0.9722 - val_loss: 2.6696 - val_accuracy: 0.5862\n",
            "Epoch 174/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9809\n",
            "Epoch 174: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.0657 - accuracy: 0.9809 - val_loss: 2.6421 - val_accuracy: 0.5931\n",
            "Epoch 175/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9705\n",
            "Epoch 175: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 83ms/step - loss: 0.0705 - accuracy: 0.9705 - val_loss: 2.5923 - val_accuracy: 0.5931\n",
            "Epoch 176/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9809\n",
            "Epoch 176: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0583 - accuracy: 0.9809 - val_loss: 2.7028 - val_accuracy: 0.5931\n",
            "Epoch 177/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9844\n",
            "Epoch 177: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.0565 - accuracy: 0.9844 - val_loss: 2.4099 - val_accuracy: 0.6207\n",
            "Epoch 178/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9792\n",
            "Epoch 178: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0611 - accuracy: 0.9792 - val_loss: 2.6828 - val_accuracy: 0.6069\n",
            "Epoch 179/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9792\n",
            "Epoch 179: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 105ms/step - loss: 0.0572 - accuracy: 0.9792 - val_loss: 2.5855 - val_accuracy: 0.6000\n",
            "Epoch 180/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9844\n",
            "Epoch 180: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0679 - accuracy: 0.9844 - val_loss: 2.4626 - val_accuracy: 0.6207\n",
            "Epoch 181/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9792\n",
            "Epoch 181: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 120ms/step - loss: 0.0550 - accuracy: 0.9792 - val_loss: 2.6835 - val_accuracy: 0.5862\n",
            "Epoch 182/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9896\n",
            "Epoch 182: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 89ms/step - loss: 0.0442 - accuracy: 0.9896 - val_loss: 2.5251 - val_accuracy: 0.6000\n",
            "Epoch 183/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9792\n",
            "Epoch 183: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0784 - accuracy: 0.9792 - val_loss: 2.4091 - val_accuracy: 0.5655\n",
            "Epoch 184/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9722\n",
            "Epoch 184: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 89ms/step - loss: 0.0778 - accuracy: 0.9722 - val_loss: 2.3258 - val_accuracy: 0.6000\n",
            "Epoch 185/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9635\n",
            "Epoch 185: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 89ms/step - loss: 0.0878 - accuracy: 0.9635 - val_loss: 2.7705 - val_accuracy: 0.5724\n",
            "Epoch 186/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9722\n",
            "Epoch 186: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0879 - accuracy: 0.9722 - val_loss: 2.5990 - val_accuracy: 0.5931\n",
            "Epoch 187/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9809\n",
            "Epoch 187: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.0647 - accuracy: 0.9809 - val_loss: 2.7074 - val_accuracy: 0.5448\n",
            "Epoch 188/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9740\n",
            "Epoch 188: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 110ms/step - loss: 0.0735 - accuracy: 0.9740 - val_loss: 2.9169 - val_accuracy: 0.5724\n",
            "Epoch 189/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9740\n",
            "Epoch 189: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 127ms/step - loss: 0.0613 - accuracy: 0.9740 - val_loss: 2.7971 - val_accuracy: 0.5724\n",
            "Epoch 190/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9809\n",
            "Epoch 190: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 110ms/step - loss: 0.0655 - accuracy: 0.9809 - val_loss: 2.3649 - val_accuracy: 0.6000\n",
            "Epoch 191/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9757\n",
            "Epoch 191: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.0714 - accuracy: 0.9757 - val_loss: 2.5341 - val_accuracy: 0.5655\n",
            "Epoch 192/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9878\n",
            "Epoch 192: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0470 - accuracy: 0.9878 - val_loss: 2.4952 - val_accuracy: 0.6069\n",
            "Epoch 193/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9844\n",
            "Epoch 193: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0401 - accuracy: 0.9844 - val_loss: 2.5759 - val_accuracy: 0.5862\n",
            "Epoch 194/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9844\n",
            "Epoch 194: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0537 - accuracy: 0.9844 - val_loss: 2.5152 - val_accuracy: 0.6000\n",
            "Epoch 195/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9757\n",
            "Epoch 195: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0662 - accuracy: 0.9757 - val_loss: 2.5025 - val_accuracy: 0.5724\n",
            "Epoch 196/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9844\n",
            "Epoch 196: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.0551 - accuracy: 0.9844 - val_loss: 2.5342 - val_accuracy: 0.6069\n",
            "Epoch 197/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9913\n",
            "Epoch 197: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 121ms/step - loss: 0.0382 - accuracy: 0.9913 - val_loss: 3.0074 - val_accuracy: 0.5793\n",
            "Epoch 198/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9809\n",
            "Epoch 198: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 120ms/step - loss: 0.0728 - accuracy: 0.9809 - val_loss: 2.4841 - val_accuracy: 0.6069\n",
            "Epoch 199/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9740\n",
            "Epoch 199: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 107ms/step - loss: 0.0643 - accuracy: 0.9740 - val_loss: 2.7133 - val_accuracy: 0.5793\n",
            "Epoch 200/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9878\n",
            "Epoch 200: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.0493 - accuracy: 0.9878 - val_loss: 2.5453 - val_accuracy: 0.5793\n",
            "Epoch 201/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9844\n",
            "Epoch 201: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0560 - accuracy: 0.9844 - val_loss: 2.5819 - val_accuracy: 0.5862\n",
            "Epoch 202/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9844\n",
            "Epoch 202: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 1s 82ms/step - loss: 0.0518 - accuracy: 0.9844 - val_loss: 2.5563 - val_accuracy: 0.6069\n",
            "Epoch 203/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9792\n",
            "Epoch 203: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0594 - accuracy: 0.9792 - val_loss: 2.7296 - val_accuracy: 0.6138\n",
            "Epoch 204/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.9757\n",
            "Epoch 204: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0600 - accuracy: 0.9757 - val_loss: 2.4551 - val_accuracy: 0.5862\n",
            "Epoch 205/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9757\n",
            "Epoch 205: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0616 - accuracy: 0.9757 - val_loss: 2.6760 - val_accuracy: 0.5862\n",
            "Epoch 206/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9740\n",
            "Epoch 206: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.0745 - accuracy: 0.9740 - val_loss: 2.8140 - val_accuracy: 0.6000\n",
            "Epoch 207/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9861\n",
            "Epoch 207: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 120ms/step - loss: 0.0458 - accuracy: 0.9861 - val_loss: 2.6139 - val_accuracy: 0.6069\n",
            "Epoch 208/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9913\n",
            "Epoch 208: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 106ms/step - loss: 0.0339 - accuracy: 0.9913 - val_loss: 2.9582 - val_accuracy: 0.6000\n",
            "Epoch 209/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9878\n",
            "Epoch 209: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0336 - accuracy: 0.9878 - val_loss: 2.7288 - val_accuracy: 0.6069\n",
            "Epoch 210/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9809\n",
            "Epoch 210: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0491 - accuracy: 0.9809 - val_loss: 2.7286 - val_accuracy: 0.6207\n",
            "Epoch 211/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9896\n",
            "Epoch 211: val_accuracy did not improve from 0.62069\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0350 - accuracy: 0.9896 - val_loss: 2.7112 - val_accuracy: 0.6207\n",
            "Epoch 212/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9740\n",
            "Epoch 212: val_accuracy improved from 0.62069 to 0.64138, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 90ms/step - loss: 0.0559 - accuracy: 0.9740 - val_loss: 2.6016 - val_accuracy: 0.6414\n",
            "Epoch 213/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9826\n",
            "Epoch 213: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.0582 - accuracy: 0.9826 - val_loss: 2.7415 - val_accuracy: 0.6069\n",
            "Epoch 214/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9809\n",
            "Epoch 214: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0487 - accuracy: 0.9809 - val_loss: 2.6644 - val_accuracy: 0.6069\n",
            "Epoch 215/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9844\n",
            "Epoch 215: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 119ms/step - loss: 0.0523 - accuracy: 0.9844 - val_loss: 2.8933 - val_accuracy: 0.6276\n",
            "Epoch 216/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9809\n",
            "Epoch 216: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.0513 - accuracy: 0.9809 - val_loss: 2.6235 - val_accuracy: 0.6207\n",
            "Epoch 217/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9844\n",
            "Epoch 217: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 108ms/step - loss: 0.0467 - accuracy: 0.9844 - val_loss: 2.9365 - val_accuracy: 0.5931\n",
            "Epoch 218/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9878\n",
            "Epoch 218: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0566 - accuracy: 0.9878 - val_loss: 2.5740 - val_accuracy: 0.6069\n",
            "Epoch 219/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9774\n",
            "Epoch 219: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0656 - accuracy: 0.9774 - val_loss: 2.5496 - val_accuracy: 0.6138\n",
            "Epoch 220/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9826\n",
            "Epoch 220: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.0563 - accuracy: 0.9826 - val_loss: 2.9492 - val_accuracy: 0.5931\n",
            "Epoch 221/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9826\n",
            "Epoch 221: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0568 - accuracy: 0.9826 - val_loss: 2.8477 - val_accuracy: 0.6000\n",
            "Epoch 222/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9774\n",
            "Epoch 222: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0652 - accuracy: 0.9774 - val_loss: 2.6555 - val_accuracy: 0.6207\n",
            "Epoch 223/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9740\n",
            "Epoch 223: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.0718 - accuracy: 0.9740 - val_loss: 2.8421 - val_accuracy: 0.6138\n",
            "Epoch 224/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1228 - accuracy: 0.9653\n",
            "Epoch 224: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 119ms/step - loss: 0.1228 - accuracy: 0.9653 - val_loss: 2.2465 - val_accuracy: 0.6345\n",
            "Epoch 225/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9792\n",
            "Epoch 225: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 124ms/step - loss: 0.0822 - accuracy: 0.9792 - val_loss: 2.9086 - val_accuracy: 0.6345\n",
            "Epoch 226/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9740\n",
            "Epoch 226: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 107ms/step - loss: 0.0692 - accuracy: 0.9740 - val_loss: 2.8689 - val_accuracy: 0.6000\n",
            "Epoch 227/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9774\n",
            "Epoch 227: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0746 - accuracy: 0.9774 - val_loss: 2.7590 - val_accuracy: 0.6345\n",
            "Epoch 228/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9844\n",
            "Epoch 228: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0556 - accuracy: 0.9844 - val_loss: 2.4202 - val_accuracy: 0.6414\n",
            "Epoch 229/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9826\n",
            "Epoch 229: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 89ms/step - loss: 0.0517 - accuracy: 0.9826 - val_loss: 2.4596 - val_accuracy: 0.6207\n",
            "Epoch 230/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9878\n",
            "Epoch 230: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0452 - accuracy: 0.9878 - val_loss: 3.0200 - val_accuracy: 0.6000\n",
            "Epoch 231/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9792\n",
            "Epoch 231: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.0708 - accuracy: 0.9792 - val_loss: 2.5120 - val_accuracy: 0.6207\n",
            "Epoch 232/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9740\n",
            "Epoch 232: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 89ms/step - loss: 0.0742 - accuracy: 0.9740 - val_loss: 2.4175 - val_accuracy: 0.6207\n",
            "Epoch 233/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9809\n",
            "Epoch 233: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 124ms/step - loss: 0.0596 - accuracy: 0.9809 - val_loss: 2.3557 - val_accuracy: 0.6345\n",
            "Epoch 234/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9844\n",
            "Epoch 234: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0524 - accuracy: 0.9844 - val_loss: 2.6711 - val_accuracy: 0.6069\n",
            "Epoch 235/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9861\n",
            "Epoch 235: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 107ms/step - loss: 0.0438 - accuracy: 0.9861 - val_loss: 2.4584 - val_accuracy: 0.6345\n",
            "Epoch 236/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9844\n",
            "Epoch 236: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.0508 - accuracy: 0.9844 - val_loss: 2.5202 - val_accuracy: 0.6276\n",
            "Epoch 237/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9844\n",
            "Epoch 237: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0431 - accuracy: 0.9844 - val_loss: 2.4480 - val_accuracy: 0.6345\n",
            "Epoch 238/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9844\n",
            "Epoch 238: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0517 - accuracy: 0.9844 - val_loss: 2.5530 - val_accuracy: 0.6207\n",
            "Epoch 239/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9878\n",
            "Epoch 239: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0400 - accuracy: 0.9878 - val_loss: 2.6192 - val_accuracy: 0.6276\n",
            "Epoch 240/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9826\n",
            "Epoch 240: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0436 - accuracy: 0.9826 - val_loss: 2.6841 - val_accuracy: 0.6069\n",
            "Epoch 241/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9809\n",
            "Epoch 241: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.0449 - accuracy: 0.9809 - val_loss: 2.5474 - val_accuracy: 0.6138\n",
            "Epoch 242/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9896\n",
            "Epoch 242: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 119ms/step - loss: 0.0405 - accuracy: 0.9896 - val_loss: 2.8230 - val_accuracy: 0.5931\n",
            "Epoch 243/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9809\n",
            "Epoch 243: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 128ms/step - loss: 0.0544 - accuracy: 0.9809 - val_loss: 2.8484 - val_accuracy: 0.6000\n",
            "Epoch 244/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9878\n",
            "Epoch 244: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 100ms/step - loss: 0.0426 - accuracy: 0.9878 - val_loss: 2.5437 - val_accuracy: 0.6000\n",
            "Epoch 245/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9722\n",
            "Epoch 245: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.0646 - accuracy: 0.9722 - val_loss: 2.6485 - val_accuracy: 0.5724\n",
            "Epoch 246/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9809\n",
            "Epoch 246: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0407 - accuracy: 0.9809 - val_loss: 2.8582 - val_accuracy: 0.5862\n",
            "Epoch 247/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9861\n",
            "Epoch 247: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 89ms/step - loss: 0.0390 - accuracy: 0.9861 - val_loss: 2.5142 - val_accuracy: 0.6069\n",
            "Epoch 248/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9896\n",
            "Epoch 248: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.0342 - accuracy: 0.9896 - val_loss: 2.6137 - val_accuracy: 0.6207\n",
            "Epoch 249/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9861\n",
            "Epoch 249: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.0418 - accuracy: 0.9861 - val_loss: 2.4493 - val_accuracy: 0.6276\n",
            "Epoch 250/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9826\n",
            "Epoch 250: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 102ms/step - loss: 0.0468 - accuracy: 0.9826 - val_loss: 2.5562 - val_accuracy: 0.6414\n",
            "Epoch 251/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9844\n",
            "Epoch 251: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 126ms/step - loss: 0.0485 - accuracy: 0.9844 - val_loss: 2.9745 - val_accuracy: 0.6069\n",
            "Epoch 252/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9826\n",
            "Epoch 252: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 121ms/step - loss: 0.0525 - accuracy: 0.9826 - val_loss: 3.2116 - val_accuracy: 0.5862\n",
            "Epoch 253/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9809\n",
            "Epoch 253: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 89ms/step - loss: 0.0516 - accuracy: 0.9809 - val_loss: 2.8373 - val_accuracy: 0.6069\n",
            "Epoch 254/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9844\n",
            "Epoch 254: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0490 - accuracy: 0.9844 - val_loss: 2.9045 - val_accuracy: 0.6138\n",
            "Epoch 255/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9826\n",
            "Epoch 255: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.0502 - accuracy: 0.9826 - val_loss: 2.8816 - val_accuracy: 0.6138\n",
            "Epoch 256/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9826\n",
            "Epoch 256: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0387 - accuracy: 0.9826 - val_loss: 3.0304 - val_accuracy: 0.6276\n",
            "Epoch 257/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9878\n",
            "Epoch 257: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 84ms/step - loss: 0.0362 - accuracy: 0.9878 - val_loss: 2.9633 - val_accuracy: 0.6414\n",
            "Epoch 258/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9896\n",
            "Epoch 258: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0248 - accuracy: 0.9896 - val_loss: 2.9554 - val_accuracy: 0.6276\n",
            "Epoch 259/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9826\n",
            "Epoch 259: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 110ms/step - loss: 0.0515 - accuracy: 0.9826 - val_loss: 2.7715 - val_accuracy: 0.6207\n",
            "Epoch 260/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9878\n",
            "Epoch 260: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 127ms/step - loss: 0.0487 - accuracy: 0.9878 - val_loss: 2.5116 - val_accuracy: 0.6207\n",
            "Epoch 261/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9861\n",
            "Epoch 261: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 112ms/step - loss: 0.0477 - accuracy: 0.9861 - val_loss: 3.0955 - val_accuracy: 0.5931\n",
            "Epoch 262/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9878\n",
            "Epoch 262: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0349 - accuracy: 0.9878 - val_loss: 2.8298 - val_accuracy: 0.6069\n",
            "Epoch 263/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9861\n",
            "Epoch 263: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.0331 - accuracy: 0.9861 - val_loss: 2.5227 - val_accuracy: 0.6276\n",
            "Epoch 264/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9792\n",
            "Epoch 264: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0536 - accuracy: 0.9792 - val_loss: 2.6535 - val_accuracy: 0.6138\n",
            "Epoch 265/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9844\n",
            "Epoch 265: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0416 - accuracy: 0.9844 - val_loss: 2.4147 - val_accuracy: 0.6345\n",
            "Epoch 266/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9757\n",
            "Epoch 266: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.0553 - accuracy: 0.9757 - val_loss: 2.5906 - val_accuracy: 0.6345\n",
            "Epoch 267/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0481 - accuracy: 0.9826\n",
            "Epoch 267: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 89ms/step - loss: 0.0481 - accuracy: 0.9826 - val_loss: 2.8657 - val_accuracy: 0.6345\n",
            "Epoch 268/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9792\n",
            "Epoch 268: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.0502 - accuracy: 0.9792 - val_loss: 2.7844 - val_accuracy: 0.6207\n",
            "Epoch 269/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9878\n",
            "Epoch 269: val_accuracy did not improve from 0.64138\n",
            "18/18 [==============================] - 2s 128ms/step - loss: 0.0452 - accuracy: 0.9878 - val_loss: 2.9561 - val_accuracy: 0.6345\n",
            "Epoch 270/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9844\n",
            "Epoch 270: val_accuracy improved from 0.64138 to 0.64828, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 109ms/step - loss: 0.0411 - accuracy: 0.9844 - val_loss: 3.0653 - val_accuracy: 0.6483\n",
            "Epoch 271/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9896\n",
            "Epoch 271: val_accuracy did not improve from 0.64828\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0351 - accuracy: 0.9896 - val_loss: 2.7450 - val_accuracy: 0.6483\n",
            "Epoch 272/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9826\n",
            "Epoch 272: val_accuracy did not improve from 0.64828\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0344 - accuracy: 0.9826 - val_loss: 2.7438 - val_accuracy: 0.6345\n",
            "Epoch 273/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9913\n",
            "Epoch 273: val_accuracy did not improve from 0.64828\n",
            "18/18 [==============================] - 2s 90ms/step - loss: 0.0327 - accuracy: 0.9913 - val_loss: 2.9248 - val_accuracy: 0.6276\n",
            "Epoch 274/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9861\n",
            "Epoch 274: val_accuracy did not improve from 0.64828\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0362 - accuracy: 0.9861 - val_loss: 2.9894 - val_accuracy: 0.6207\n",
            "Epoch 275/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9844\n",
            "Epoch 275: val_accuracy did not improve from 0.64828\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0411 - accuracy: 0.9844 - val_loss: 3.0947 - val_accuracy: 0.6207\n",
            "Epoch 276/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9896\n",
            "Epoch 276: val_accuracy did not improve from 0.64828\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0296 - accuracy: 0.9896 - val_loss: 2.7854 - val_accuracy: 0.6414\n",
            "Epoch 277/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9913\n",
            "Epoch 277: val_accuracy did not improve from 0.64828\n",
            "18/18 [==============================] - 2s 126ms/step - loss: 0.0310 - accuracy: 0.9913 - val_loss: 2.9357 - val_accuracy: 0.6483\n",
            "Epoch 278/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9861\n",
            "Epoch 278: val_accuracy did not improve from 0.64828\n",
            "18/18 [==============================] - 2s 128ms/step - loss: 0.0392 - accuracy: 0.9861 - val_loss: 2.7490 - val_accuracy: 0.6345\n",
            "Epoch 279/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9913\n",
            "Epoch 279: val_accuracy did not improve from 0.64828\n",
            "18/18 [==============================] - 2s 94ms/step - loss: 0.0300 - accuracy: 0.9913 - val_loss: 2.8873 - val_accuracy: 0.6207\n",
            "Epoch 280/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9861\n",
            "Epoch 280: val_accuracy improved from 0.64828 to 0.65517, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 90ms/step - loss: 0.0314 - accuracy: 0.9861 - val_loss: 2.6300 - val_accuracy: 0.6552\n",
            "Epoch 281/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9896\n",
            "Epoch 281: val_accuracy did not improve from 0.65517\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0276 - accuracy: 0.9896 - val_loss: 3.1398 - val_accuracy: 0.6069\n",
            "Epoch 282/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9844\n",
            "Epoch 282: val_accuracy did not improve from 0.65517\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.0394 - accuracy: 0.9844 - val_loss: 2.6930 - val_accuracy: 0.6345\n",
            "Epoch 283/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9861\n",
            "Epoch 283: val_accuracy did not improve from 0.65517\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0325 - accuracy: 0.9861 - val_loss: 2.8273 - val_accuracy: 0.6276\n",
            "Epoch 284/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9878\n",
            "Epoch 284: val_accuracy did not improve from 0.65517\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0297 - accuracy: 0.9878 - val_loss: 2.9338 - val_accuracy: 0.6207\n",
            "Epoch 285/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9913\n",
            "Epoch 285: val_accuracy did not improve from 0.65517\n",
            "18/18 [==============================] - 2s 106ms/step - loss: 0.0277 - accuracy: 0.9913 - val_loss: 2.8741 - val_accuracy: 0.6207\n",
            "Epoch 286/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9844\n",
            "Epoch 286: val_accuracy improved from 0.65517 to 0.66207, saving model to best_model.h5\n",
            "18/18 [==============================] - 2s 123ms/step - loss: 0.0398 - accuracy: 0.9844 - val_loss: 2.9967 - val_accuracy: 0.6621\n",
            "Epoch 287/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9861\n",
            "Epoch 287: val_accuracy did not improve from 0.66207\n",
            "18/18 [==============================] - 2s 124ms/step - loss: 0.0267 - accuracy: 0.9861 - val_loss: 2.8522 - val_accuracy: 0.6414\n",
            "Epoch 288/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9896\n",
            "Epoch 288: val_accuracy did not improve from 0.66207\n",
            "18/18 [==============================] - 2s 86ms/step - loss: 0.0271 - accuracy: 0.9896 - val_loss: 3.0116 - val_accuracy: 0.6207\n",
            "Epoch 289/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9861\n",
            "Epoch 289: val_accuracy did not improve from 0.66207\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.0303 - accuracy: 0.9861 - val_loss: 3.1144 - val_accuracy: 0.6207\n",
            "Epoch 290/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9878\n",
            "Epoch 290: val_accuracy did not improve from 0.66207\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0316 - accuracy: 0.9878 - val_loss: 3.0936 - val_accuracy: 0.6345\n",
            "Epoch 291/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9861\n",
            "Epoch 291: val_accuracy did not improve from 0.66207\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0332 - accuracy: 0.9861 - val_loss: 3.0369 - val_accuracy: 0.6345\n",
            "Epoch 292/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9896\n",
            "Epoch 292: val_accuracy did not improve from 0.66207\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0257 - accuracy: 0.9896 - val_loss: 2.8651 - val_accuracy: 0.6414\n",
            "Epoch 293/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9861\n",
            "Epoch 293: val_accuracy did not improve from 0.66207\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0300 - accuracy: 0.9861 - val_loss: 3.0586 - val_accuracy: 0.6276\n",
            "Epoch 294/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9826\n",
            "Epoch 294: val_accuracy did not improve from 0.66207\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0319 - accuracy: 0.9826 - val_loss: 2.7982 - val_accuracy: 0.6483\n",
            "Epoch 295/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9844\n",
            "Epoch 295: val_accuracy did not improve from 0.66207\n",
            "18/18 [==============================] - 2s 121ms/step - loss: 0.0270 - accuracy: 0.9844 - val_loss: 3.0009 - val_accuracy: 0.6345\n",
            "Epoch 296/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9826\n",
            "Epoch 296: val_accuracy did not improve from 0.66207\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.0586 - accuracy: 0.9826 - val_loss: 2.8155 - val_accuracy: 0.6345\n",
            "Epoch 297/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9826\n",
            "Epoch 297: val_accuracy did not improve from 0.66207\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0302 - accuracy: 0.9826 - val_loss: 3.0335 - val_accuracy: 0.6414\n",
            "Epoch 298/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9844\n",
            "Epoch 298: val_accuracy did not improve from 0.66207\n",
            "18/18 [==============================] - 2s 88ms/step - loss: 0.0324 - accuracy: 0.9844 - val_loss: 2.8526 - val_accuracy: 0.6276\n",
            "Epoch 299/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9861\n",
            "Epoch 299: val_accuracy did not improve from 0.66207\n",
            "18/18 [==============================] - 2s 85ms/step - loss: 0.0381 - accuracy: 0.9861 - val_loss: 2.6823 - val_accuracy: 0.6276\n",
            "Epoch 300/300\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9878\n",
            "Epoch 300: val_accuracy did not improve from 0.66207\n",
            "18/18 [==============================] - 2s 87ms/step - loss: 0.0305 - accuracy: 0.9878 - val_loss: 3.1009 - val_accuracy: 0.6345\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Tentukan callback ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint('best_model.h5',\n",
        "                             monitor='val_accuracy',  # Gunakan metrik yang relevan, misalnya 'val_loss' atau 'val_accuracy'\n",
        "                             save_best_only=True,     # Hanya simpan model terbaik\n",
        "                             mode='max',              # Mode 'max' untuk akurasi, 'min' untuk loss\n",
        "                             verbose=1)\n",
        "\n",
        "# Latih model dengan menggunakan callback ModelCheckpoint\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=300,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Win2bWp3xsMr"
      },
      "source": [
        "### plot akurasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "_lLiSiC4xsMs",
        "outputId": "b1a511ad-2361-4838-c319-295e47ac82b1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Menyimpan model\n",
        "# model.save('chatbot-new-.h5')\n",
        "\n",
        "# Memuat model dari file H5\n",
        "loaded_model = load_model('best_model.h5')\n"
      ],
      "metadata": {
        "id": "BUj_UrvXxqPM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-c2pNHC0xsMs"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "# Load stopwords from file\n",
        "stop_words = set()\n",
        "with open('stopword_id.txt', 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        stop_words.add(line.strip())\n",
        "\n",
        "\n",
        "# Normalization rule for replacing \"haid\" with \"menstruasi\"\n",
        "normalization_rules = {'haid': 'menstruasi'}\n",
        "\n",
        "def preprocess_input(user_input):\n",
        "    # Convert to lowercase\n",
        "    user_input = user_input.lower()\n",
        "\n",
        "    # Normalize using rules\n",
        "    for key, value in normalization_rules.items():\n",
        "        user_input = user_input.replace(key, value)\n",
        "\n",
        "    # Remove characters not in [a-zA-Z0-9\\s]\n",
        "    user_input = re.sub(r'[.,’\"\\'-?:!;]', '', user_input)\n",
        "\n",
        "    # Remove punctuation\n",
        "    user_input = user_input.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = user_input.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Join the words\n",
        "    preprocessed_text = ' '.join(words)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    preprocessed_text = re.sub(' +', ' ', preprocessed_text)\n",
        "\n",
        "    return preprocessed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcTWM2RPxsMt"
      },
      "source": [
        "### check output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "M9Q4zHdLxsMt",
        "outputId": "d2eceb9a-7797-4c69-c9ab-775fb1c9f7ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hi\n",
            "hi\n",
            "1/1 [==============================] - 1s 571ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tags: salam\n",
            "Chatbot: Hai! femibot di sini. Mau tau informasi tentang apa nih?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-07091412cf28>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mpreprocessed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# preprocessed_text = user_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "while True:\n",
        "    user_input = input('You: ')\n",
        "    preprocessed_text = preprocess_input(user_input)\n",
        "    # preprocessed_text = user_input\n",
        "    print(preprocessed_text)\n",
        "    if preprocessed_text.lower() == 'goodbye':\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    try:\n",
        "        input_seq = tokenizer.texts_to_sequences([preprocessed_text])\n",
        "        input_seq = pad_sequences(input_seq, maxlen=max_len)\n",
        "        predicted_label_seq = model.predict(input_seq)\n",
        "        predicted_label = label_encoder.inverse_transform([predicted_label_seq.argmax(axis=-1)])[0]\n",
        "        print(\"tags:\", predicted_label)\n",
        "\n",
        "        # Pilih respons dari kategori tag\n",
        "        responses_for_tag = responses.get(predicted_label, [\"Chatbot: Maaf, saya tidak memahami maksud Anda.\"])\n",
        "        response = random.choice(responses_for_tag)\n",
        "\n",
        "        print(\"Chatbot:\", response)\n",
        "    except:\n",
        "        print(\"Chatbot: Maaf, saya tidak memahami maksud Anda.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalasi tensorflowjs\n",
        "!pip install tensorflowjs\n",
        "\n",
        "# Impor modul yang diperlukan\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflowjs as tfjs\n",
        "\n",
        "# Muat model Keras\n",
        "model_path = '/content/best_model.h5'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Konversi model ke format TensorFlow.js\n",
        "tfjs.converters.save_keras_model(model, '/content/tfjs_model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEpGgiwuM6mV",
        "outputId": "fb3be5a3-a49b-4878-9224-c476dba192bd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.10/dist-packages (4.14.0)\n",
            "Requirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.7.5)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (6.1.1)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.20)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.20+cuda11.cudnn86)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.15.0.post1)\n",
            "Requirement already satisfied: tensorflow-decision-forests>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.8.1)\n",
            "Requirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.15.0)\n",
            "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (23.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.23.5)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.0.7)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.7)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (13.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (1.11.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (16.0.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (1.5.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.42.0)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.16.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.1)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.7)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.5.2)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.5.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2023.3.post1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax>=0.7.2->tensorflowjs) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax>=0.7.2->tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (2.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2023.6.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.2.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}